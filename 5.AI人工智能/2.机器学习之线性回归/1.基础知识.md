# 1. 基础知识

线性回归是机器学习中`有监督`机器学习下的一种算法.

回归问题主要关注的是因变量(需要预测的值,可以是一个也可以是多个)和一个或多个数值型的因变量之间的关系
- 需要预测的值: 即目标变量,target,y,`连续值`预测变量
- 影响目标变量的因素: x1,...xn,可以是连续值,也可以是离散值
- 因变量和自变量之间的关系: 即`模型`,modal,是我们要求解的

## 1.1 连续值

数值连续

## 1.2 离散值

数值离散

### 1.3 简单线性回归

说白了,算法就是公式,简单线性回归属于一个算法,对应的公式:

```
y = wx + b
```

这个公式中，y是目标变量即未来要预测的值，×是影响y的因素，w，b 是公式上的参数即要求的模型。其实b 就是咱们的截距，w 就是斜率嘛！所以很明显如果模型求出来了，未来影响 y值的未知数就是一个x值，也可以说影响y值 的因素只有一个，所以这是就叫简单线性回归的原因。

**同时可以发现从×到y的计算，x只是一次方，所以这是算法叫线性回归的原因**。其实，大家上小学时就已经会解这种一元一次方程了。为什么那个时候不叫人工智能算法呢？**因为人工智能算法要求的是最优解！**

## 1.4 最优解
- Actual value：**真实值，一般使用y表示。**
- Predicted value：**预测值**，是把已知的×带入到公式里面和猜出来的参数w，b计算得到的，**一般使用 ŷ 表示**。
- Error：**误差**，预测值和真实值的差距，一般使用：表示。
- **最优解**：尽可能的找到一个模型使得整体的误差最小，整体的误差通常叫做损失 Loss。
- Loss：整体的误差，Loss 通过损失函数Loss function 计算得到。

![简单线性回归](<../img/dyxxhg1.png>)

## 1.5 多元线性回归

现实生活中，往往影响结果y的因素不止一个，这时×就从一个变成了n个，X1...Xn，同时简单线性回归的公式也就不在适用了。**多元线性回归公式**如下：
```
ŷ = w1X1 + w2X2 + ...+ wnXn + b

b是截距也可以用w0来表示

ŷ = w1X1 + w2X2 + ...+ wnXn + w0
ŷ = w1X1 + w2X2 + ...+ wnXn + w0 * 1
```

使用向量来表示，X表示所有的变量，是一维向量；W表示所有的系数（包含w0），是一维向量，根据向量乘法规律，可以这么写：

ŷ = $W^T$ X

![多元线性回归](<../img/dyxxhg2.png>)
